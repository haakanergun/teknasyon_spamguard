{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a88167",
   "metadata": {},
   "source": [
    "### 1. Veri Setini Yükleme ve Keşif\n",
    "\n",
    "Veri setimiz `sms_spam_train.csv` dosyasından yüklenecek. Dosyada her satırda bir SMS mesajı ve mesajın \"ham\" veya \"spam\" olarak etiketlendiği iki sütun (Message, Label) bulunuyor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93aff52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hakan\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whats upp ac enisteee: -) senin memlekette bul...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kampanya, VakifBank Worldcardla 31 Mayisa kada...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I wont. So wat's wit the guys</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bu arada son dakika evi düşürdük</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No that just means you have a fat head</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message Label\n",
       "0  Whats upp ac enisteee: -) senin memlekette bul...   ham\n",
       "1  Kampanya, VakifBank Worldcardla 31 Mayisa kada...  spam\n",
       "2                      I wont. So wat's wit the guys   ham\n",
       "3                   Bu arada son dakika evi düşürdük   ham\n",
       "4             No that just means you have a fat head   ham"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını yükleyin\n",
    "df = pd.read_csv(\"sms_spam_train.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6434219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Message  10000 non-null  object\n",
      " 1   Label    10000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 156.4+ KB\n",
      "Label\n",
      "ham     6621\n",
      "spam    3379\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "print(df['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab343e",
   "metadata": {},
   "source": [
    "### 2. Eğitim ve Validasyon Verilerinin Oluşturulması\n",
    "\n",
    "Sağlanan veriyi %80 eğitim ve %20 validasyon olacak şekilde böleceğiz. Ayrıca, etiket dengesini korumak için stratified split kullanacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a97e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim verisi örnek sayısı: 8000\n",
      "Validasyon verisi örnek sayısı: 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Label'])\n",
    "\n",
    "print(\"Eğitim verisi örnek sayısı:\", len(train_df))\n",
    "print(\"Validasyon verisi örnek sayısı:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36fec6",
   "metadata": {},
   "source": [
    "### 3. Veri Setini Prompt Formatına Dönüştürme\n",
    "\n",
    "Mistral modelini instruction fine-tuning ile eğiteceğimiz için, her örneği aşağıdaki formatta hazırlayacağız:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb6159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aşağıdaki SMS mesajını spam mı yoksa ham mı olarak sınıflandırınız?\n",
      "Mesaj: I don't run away frm u... I walk slowly &amp; it kills me that u don't care enough to stop me...\n",
      "Cevap: ham\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(row):\n",
    "    \"\"\"\n",
    "    Her bir örnek için prompt formatını oluşturur.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Aşağıdaki SMS mesajını spam mı yoksa ham mı olarak sınıflandırınız?\\n\"\n",
    "        f\"Mesaj: {row['Message']}\\n\"\n",
    "        f\"Cevap: {row['Label']}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Eğitim ve validasyon verileri için prompt'ları oluşturun\n",
    "train_prompts = train_df.apply(format_prompt, axis=1).tolist()\n",
    "val_prompts = val_df.apply(format_prompt, axis=1).tolist()\n",
    "\n",
    "# Örnek bir prompt kontrolü\n",
    "print(train_prompts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c60b6",
   "metadata": {},
   "source": [
    "Şimdi, Hugging Face `datasets` kütüphanesiyle kullanılacak formata (örneğin, bir sözlük içinde \"text\" anahtarı) dönüştürelim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010e20af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Aşağıdaki SMS mesajını spam mı yoksa ham mı olarak sınıflandırınız?\\nMesaj: I don't run away frm u... I walk slowly &amp; it kills me that u don't care enough to stop me...\\nCevap: ham\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_prompts})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_prompts})\n",
    "\n",
    "# İlk örneği kontrol edelim\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b72ef",
   "metadata": {},
   "source": [
    "### 4. Model ve Tokenizer’ın Kurulumu\n",
    "\n",
    "Mistral-7B-Instruct modelini Hugging Face üzerinden indirip, 4-bit quantization ve gradient checkpointing ile model belleğini optimize ediyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe8afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9ca433",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m local_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_model_path)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_model_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "local_model_path = \"mistral-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00ae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d95d854c9b4e8ba4ca463dc82d26a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hakan\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hakan\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10df03117213436fbaf5fe008cd3ad4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48499032b6a4a7c9646e5193230d2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd52a86598f1428b9ae2e34642c80760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import prepare_model_for_kbit_training  # PEFT içindeki yardımcı fonksiyonu kullanıyoruz\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Modeli 4-bit quantization ile yükleyin\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13326729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
